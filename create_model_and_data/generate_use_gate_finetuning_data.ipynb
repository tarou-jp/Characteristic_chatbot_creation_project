{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llamaindexを用いた検索結果をプロンプトに挿入するのかを判断させる言語モデルの作成のためのファインチューニング用データを用意するためのソースコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "title = \"NoGame\"\n",
    "\n",
    "# 環境変数の設定\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"-------伏字-------\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "# ドキュメントの読み込み\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\"row_anime_voices\\scenes_summary.txt\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# ノードパーサーの準備\n",
    "text_splitter = SentenceSplitter(\n",
    "    chunk_overlap = 0,\n",
    "    paragraph_separator=\"[SEP]\",\n",
    ")\n",
    "node_parser = SimpleNodeParser.from_defaults(\n",
    "    text_splitter=text_splitter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import MetadataMode\n",
    "import json\n",
    "\n",
    "# ドキュメントをコーパスに変換\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "corpus = {node.node_id: node.get_content(metadata_mode=MetadataMode.NONE) for node in nodes}\n",
    "\n",
    "# コーパスの保存\n",
    "with open(f\"RAG/{title}_summary_corpus.json\", \"w+\",encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既にファイル作成済みの場合はこっち\n",
    "import json\n",
    "\n",
    "with open(f\"RAG/{title}_summary_corpus.json\",\"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "from llama_index.llms import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 合成データの作成関数\n",
    "def generate_queries(\n",
    "    corpus,\n",
    "    num_questions_per_chunk=10,\n",
    "    prompt_template=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    prompt_template = prompt_template or \"\"\"\\\n",
    "    文脈は以下のとおりです。\n",
    "\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "\n",
    "    あなたは教師です。 あなたの仕事は、試験問題を作成することです。\n",
    "    日本のアニメ作品に文脈から重要な事実を捉える{num_questions_per_chunk} 個の質問を、以下の条件で作成します。\n",
    "    - 質問は必ず提供された文脈に限定\n",
    "    - 質問は必ず日本語で記述\n",
    "    - 代名詞は絶対に使用しない\n",
    "    - 質問は文書全体にわたって本質的に多様である必要がある\n",
    "    \"\"\"\n",
    "\n",
    "    queries = {}\n",
    "    queries_relevant_docs = {}\n",
    "    for node_id, text in tqdm(corpus.items()):\n",
    "        query = prompt_template.format(context_str=text, num_questions_per_chunk=num_questions_per_chunk)\n",
    "        response = llm.complete(query)\n",
    "\n",
    "        result = str(response).strip().split(\"\\n\")\n",
    "        questions = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "        ]\n",
    "        questions = [question for question in questions if len(question) > 0]\n",
    "\n",
    "        for question in questions:\n",
    "            question_id = str(uuid.uuid4())\n",
    "            queries[question_id] = question\n",
    "            queries_relevant_docs[question_id] = [node_id]\n",
    "    return queries, queries_relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合成データの作成\n",
    "queries, queries_relevant_docs = generate_queries(corpus,num_questions_per_chunk=20)\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"final_character_DB\\NoGame_queries.json\",\"w\") as f:\n",
    "    json.dump(queries,f, ensure_ascii=False)\n",
    "\n",
    "with open(r\"final_character_DB\\NoGame_queries_relevant_docs.json\",\"w\") as f:\n",
    "    json.dump(queries_relevant_docs,f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_list=[]\n",
    "queries_id_list=[]\n",
    "\n",
    "for key, value in queries.items():\n",
    "    queries_list.append(value)\n",
    "    queries_id_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "queries_id_real_list = queries_id_list[:]\n",
    "shuffled_queries_id_list = []\n",
    "\n",
    "for i in range(len(queries_id_real_list)):\n",
    "    if (i%2==1):\n",
    "        shuffled_queries_id_list.append(random.choice(queries_id_list))\n",
    "    else:\n",
    "        shuffled_queries_id_list.append(queries_id_list[i])\n",
    "\n",
    "\n",
    "with open(r\"train_dataset\\train_dataset_for_usegate.csv\",\"w\") as f:\n",
    "    for i in range(len(queries_list)):\n",
    "        if (shuffled_queries_id_list[i] == queries_id_real_list[i]):\n",
    "            answer = corpus[queries_relevant_docs[shuffled_queries_id_list[i]][0]]\n",
    "            answer = answer.replace(\"\\n\",\"\")\n",
    "            answer = answer.replace(\",\",\"、\")\n",
    "            query = queries_list[i]\n",
    "            query = query.replace(\",\",\"、\")\n",
    "            f.write(query+\",\"+answer+\",0\\n\")\n",
    "        else:\n",
    "            answer = corpus[queries_relevant_docs[shuffled_queries_id_list[i]][0]]\n",
    "            answer = answer.replace(\"\\n\",\"\")\n",
    "            answer = answer.replace(\",\",\"、\")\n",
    "            query = queries_list[i]\n",
    "            query = query.replace(\",\",\"、\")\n",
    "            f.write(query+\",\"+answer+\",1\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jouhou",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
